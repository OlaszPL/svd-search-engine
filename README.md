# SVD Search Engine ğŸ”

SVD Search Engine to silnik wyszukiwania wykorzystujÄ…cy metodÄ™ Singular Value Decomposition (SVD) do analizy i wyszukiwania podobnych artykuÅ‚Ã³w. DziÄ™ki redukcji wymiarowoÅ›ci oraz ekstrakcji kluczowych cech tekstu, umoÅ¼liwia efektywne porÃ³wnywanie treÅ›ci. System operuje na wektorach cech typu _bag_of_words_, ktÃ³re odzwierciedlajÄ… czÄ™stoÅ›Ä‡ wystÄ™powania sÅ‚Ã³w kluczowych w artykuÅ‚ach.

RozwiÄ…zanie pozwala na przeszukiwanie artykuÅ‚Ã³w pochodzÄ…cych z dowolnego zrzutu danych z _Wikipedii_ (w formacie _.xml_).

![](img/front.png)

## UÅ¼ytkowanie

### Wymagania techniczne

- Python 3.13+
- Biblioteki: _numpy, hnswlib, nltk, scikit-learn, streamlit, tqdm_

> [!IMPORTANT]
> Wiele operacji stosowanych w wyszukiwarce intenstywnie korzysta z zasobÃ³w pamiÄ™ci operacyjnej, dlatego zalecane jest ponad 16GB pamiÄ™ci RAM (w szczegÃ³lnoÅ›Ä‡i dla duÅ¼ej liczby _k_ stosowanej w SVD).

### Wymagania wstÄ™pne

**1. Sklonuj repozytorium:**
```bash
git clone https://github.com/OlaszPL/svd-search-engine.git
```

**2. PrzejdÅº do katalogu z kodem ÅºrÃ³dÅ‚owym:**
```bash
cd svd-search-engine
```

**3. Upewnij siÄ™, Å¼e masz zainstalowane wymagane biblioteki. MoÅ¼na je zainstalowaÄ‡ za pomocÄ… pliku `requirements.txt`:**
```bash
pip install -r requirements.txt
```

### Przygotowanie danych

**1. Pobierz dowolny zrzut danych z Wikipedii (w jÄ™zyku angielskim) [z tej strony](https://dumps.wikimedia.org/backup-index.html), np. zrzut [_simple_english_](https://dumps.wikimedia.org/simplewiki/20250420/simplewiki-20250420-pages-articles-multistream.xml.bz2) (w formacie _.xml.bz2_).**

**2. PrzejdÅº do folderu ``data``, wypakuj pobrane archiwum do folderu ``input``, zmieÅ„ nazwÄ™ pliku na ``wiki.xml``. NastÄ™pnie uruchom:**
```bash
python ./create_wiki_db.py
```
Zaczekaj na utworzenie bazy danych.

Plik _.xml_ parse'owany jest przy pomocy [mojego forka](https://github.com/OlaszPL/wikiextractor/) biblioteki _wikiextractor_, gdzie naprawiÅ‚em wsparcie dla Pythona 3.13, systemu Windows, funkcjonowanie zapisu do plikÃ³w _json_ oraz przetwarzanie list punktowanych. Pakiet zostaÅ‚ zamieszczony w folderze `data`, nie trzeba podejmowaÄ‡ dodatkowych krokÃ³w.

**3. Uruchom tworzenie _bag_of_words_, a nastÄ™pnie _term_by_document_:**
```bash
python ./bag_of_words.py

python ./term-by-document.py
```

>[!NOTE]
> Obie operacje wymagajÄ… czasu na inicjalizacjÄ™ procesÃ³w oraz przetworzenie wszystkich artykuÅ‚Ã³w.

## Uruchamianie aplikacji

**PrzejdÅº do katalogu gÅ‚Ã³wnego a nastÄ™pnie wywoÅ‚aj:**
```bash
streamlit run main.py
```

Wybranie przy pomocy suwaka _k_ utworzy SVD dla macierzy oraz indeks HNSW. CzynnoÅ›Ä‡ ta jest jednorazowa dla wybranego k (pliki sÄ… zapisywane w katalogu `data/objects/SVD`).

## RozwiÄ…zania przyjÄ™te podczas implementacji

### Pozyskiwane dane

Dane pochodzÄ… z oficjalnych zrzutÃ³w Wikipedii w formacie _.xml_ (do testÃ³w skorzystaÅ‚em z _simplewiki_). Plik _.xml_ parse'owany jest przy pomocy [mojego forka](https://github.com/OlaszPL/wikiextractor/) biblioteki _wikiextractor_, gdzie naprawiÅ‚em wsparcie dla Pythona 3.13, systemu Windows, funkcjonowanie zapisu do plikÃ³w _json_ oraz przetwarzanie list punktowanych. NastÄ™pnie tworzona jest baza danych SQLite - dla wygodnego dostÄ™pu w pÃ³Åºniejszych etapach przetwarzania.

### Utworzenie zbioru sÅ‚Ã³w _bag_of_words_

ArtykuÅ‚y przetwarzam wieloprocesowo z wykorzystaniem ``ProcessPoolExecutor``. Tekst kaÅ¼dego z artykuÅ‚Ã³w jest tokenizowany przy pomocy ``word_tokenize`` z biblioteki ``nltk``. NastÄ™pnie kaÅ¼de ze sÅ‚Ã³w jest sprowadzane do rdzenia przy pomocy algorytmu ``PorterStemmer`` (jednoczeÅ›nie przetwarzane sÄ… do maÅ‚ych liter) oraz dodawane do zbioru sÅ‚Ã³w o ile nie jest zawarte w zbiorze ``stopwords``. ZbiÃ³r ten jest uniÄ… terminÃ³w wczytanych z pliku tekstowego, z biblioteki ``nltk`` oraz znakÃ³w interpunkcyjnych. Finalnie zapisywany jest sÅ‚ownik mapujÄ…cy sÅ‚owa na kolejne indeksy.

### Budowanie macierzy z wektorÃ³w _term_by_document_

Proces jest podobny do generowania zbioru _bag_of_words_ z tÄ… rÃ³Å¼nicÄ…, Å¼e tym razem dla kaÅ¼dego z artykuÅ‚Ã³w tworzony jest sÅ‚ownik, ktÃ³ry dla kaÅ¼dego indeksu sÅ‚owa (indeksy pochodzÄ… ze sÅ‚ownika mapujÄ…cego) zlicza liczbÄ™ jego wystÄ…pieÅ„ w caÅ‚ym tekÅ›cie. NastÄ™pnie na podstawie tych sÅ‚ownikÃ³w buduje siÄ™ macierz rzadkÄ… (``csr``) zawierajÄ…cÄ… wektory wystÄ…pieÅ„ sÅ‚Ã³w dla kaÅ¼dego z artykuÅ‚Ã³w.

NastÄ™pnie przy pomocy ``TfidfTransfmormer`` aplikowane sÄ… TF-IDF (Term Frequencyâ€“Inverse Document Frequency) oraz normalizacja w normie _l2_. Wykorzystanie TF-IDF pomaga tutaj: 

+ WyrÃ³Å¼niÄ‡ istotne sÅ‚owa â€“ podnosi wagÄ™ sÅ‚Ã³w, ktÃ³re czÄ™sto wystÄ™pujÄ… w danym dokumencie, ale rzadko w innych.
+ RedukowaÄ‡ wpÅ‚yw popularnych sÅ‚Ã³w â€“ obniÅ¼a wagÄ™ sÅ‚Ã³w bardzo czÄ™sto wystÄ™pujÄ…cych w caÅ‚ym korpusie (np. "i", "oraz", "the"), ktÃ³re nie niosÄ… istotnej informacji.

Finalnie macierz zapisywana jest do pliku.

### SVD

Istnieje moÅ¼liwoÅ›Ä‡ opcjonalnego usuniÄ™cia szumu z macierzy A poprzez SVD (Singular Value Decomposition) i low rank approximation. Po ustawieniu preferowanej wartoÅ›ci ``k`` tworzone jest SVD przy pomocy bibliotecznej funkcji ``TruncatedSVD``, ktÃ³ra dostosowana jest do macierzy rzadkich, na ktÃ³rych zastosowano TF-IDF. Bardzo dobrze redukuje tutaj wymiarowoÅ›Ä‡. Wygenerowana macierz jest zapisywana do pÃ³Åºniejszego wykorzystania.

### Wyszukiwanie

Wyszukiwanie rozpoczyna siÄ™ od przetworzenia wprowadzonego tekstu w wektor o identycznej postaci co te umieszczone w macierzy (w przypadku wykorzystania macierzy z SVD wektor jest dodatrkowo transformowany).

NastÄ™pnie wyszukiwanie odbywaÄ‡ siÄ™ moÅ¼e na 2 sposoby:

1. Z wykorzystaniem wartoÅ›ci _cosinusa_ miÄ™dzy wektorami oraz posortowaniem malejÄ…co poprzez ``np.argsort``,
2. Przy pomocy ANN (Approximate Nearest Neighbors) z wykorzystaniem bibliotecznego algorytmu HNSW (Hierarchical Navigable Small World). Ze wzglÄ™du na dostÄ™pne zasoby systemowe, ANN moÅ¼na zastosowaÄ‡ tylko na macierzach z SVD. Indeks _hnswlib_ tworzony jest podczas generowania SVD i wczytywany po utworzeniu obiektu ``Search``.

### Frontend

Frontent wyszukiwarki napisany zostaÅ‚ z wykorzystaniem frameworka ``streamlit``.

## PorÃ³wnanie dziaÅ‚ania programu bez usuwania szumu i z usuwaniem szumu - dla rÃ³Å¼nych wartoÅ›ci ``k``